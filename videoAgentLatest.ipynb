{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "215a6f0f729c4c41937c1d46425a79d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc0e0485e66c4d61a8880b3a459d96c0",
              "IPY_MODEL_47e84abaf97c4d4baaee6d1b707588e9",
              "IPY_MODEL_bcb586601ba84d1aad851d4825f2b218"
            ],
            "layout": "IPY_MODEL_966e96d4e4a448a2b6a5df5f35676d66"
          }
        },
        "fc0e0485e66c4d61a8880b3a459d96c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6cab3d77ee04f60bd251b0a74e75421",
            "placeholder": "​",
            "style": "IPY_MODEL_aac258b9c7354b319a3f45bc93888f70",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "47e84abaf97c4d4baaee6d1b707588e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48028d23f83d42ba92f9d01198927969",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f39992ec5b424e5eb0c19414ffc97531",
            "value": 2
          }
        },
        "bcb586601ba84d1aad851d4825f2b218": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f42f1cb622d84a03b87397331b6970c8",
            "placeholder": "​",
            "style": "IPY_MODEL_4f219fd2e7a84fa49263f9c729bd07ee",
            "value": " 2/2 [00:02&lt;00:00,  1.05s/it]"
          }
        },
        "966e96d4e4a448a2b6a5df5f35676d66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6cab3d77ee04f60bd251b0a74e75421": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aac258b9c7354b319a3f45bc93888f70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48028d23f83d42ba92f9d01198927969": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f39992ec5b424e5eb0c19414ffc97531": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f42f1cb622d84a03b87397331b6970c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f219fd2e7a84fa49263f9c729bd07ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7081e80495d0401aab089de22836c499": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_907431dc830342a7bc216b9657cb942d",
              "IPY_MODEL_cb710d330ca044888e5e24cca0f27751",
              "IPY_MODEL_6f358c3522c04464880e06c6b6145db0"
            ],
            "layout": "IPY_MODEL_e459d41bf78844149fad4563434fa594"
          }
        },
        "907431dc830342a7bc216b9657cb942d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_622f52c2e9504e9a97e730bad148a64e",
            "placeholder": "​",
            "style": "IPY_MODEL_d1ad94b0fa7d43d68fde842e7cd30133",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "cb710d330ca044888e5e24cca0f27751": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8566ca5225e4be194ff33a4be4a4ff5",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f070f6e1972f4370afb02e545f1e0854",
            "value": 3
          }
        },
        "6f358c3522c04464880e06c6b6145db0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbd22e55dfb34834bebf8b8e0f059e8c",
            "placeholder": "​",
            "style": "IPY_MODEL_fd2ef6269c0b4f3bbfc20f42a7a32a67",
            "value": " 3/3 [00:04&lt;00:00,  1.53s/it]"
          }
        },
        "e459d41bf78844149fad4563434fa594": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "622f52c2e9504e9a97e730bad148a64e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1ad94b0fa7d43d68fde842e7cd30133": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8566ca5225e4be194ff33a4be4a4ff5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f070f6e1972f4370afb02e545f1e0854": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fbd22e55dfb34834bebf8b8e0f059e8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd2ef6269c0b4f3bbfc20f42a7a32a67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M-NWYBBp3TjU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "462b583d-7a42-42ca-df55-cfe6986cbb0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: langgraph 0.2.76\n",
            "Uninstalling langgraph-0.2.76:\n",
            "  Successfully uninstalled langgraph-0.2.76\n",
            "Found existing installation: langchain 0.3.27\n",
            "Uninstalling langchain-0.3.27:\n",
            "  Successfully uninstalled langchain-0.3.27\n",
            "Found existing installation: langchain-core 0.3.75\n",
            "Uninstalling langchain-core-0.3.75:\n",
            "  Successfully uninstalled langchain-core-0.3.75\n",
            "Found existing installation: langchain-community 0.3.28\n",
            "Uninstalling langchain-community-0.3.28:\n",
            "  Successfully uninstalled langchain-community-0.3.28\n",
            "Found existing installation: huggingface-hub 0.34.4\n",
            "Uninstalling huggingface-hub-0.34.4:\n",
            "  Successfully uninstalled huggingface-hub-0.34.4\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y langgraph langchain langchain-core langchain-community huggingface-hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- GPU stack (cu121) hard reset: pin Numpy 1.x + SciPy + scikit-learn, PyTorch 2.3.0, CUDA runtime ---\n",
        "\n",
        "import sys, subprocess, os, site, textwrap\n",
        "\n",
        "def pip(*args):\n",
        "    print(\">> pip\", *args)\n",
        "    return subprocess.check_call([sys.executable, \"-m\", \"pip\", *args])\n",
        "\n",
        "# 0) Purge conflicting wheels (incl. sklearn which drags SciPy/Numpy)\n",
        "pip(\"uninstall\",\"-y\",\n",
        "    \"torch\",\"torchvision\",\"torchaudio\",\n",
        "    \"numpy\",\"scipy\",\"scikit-learn\",\n",
        "    \"opencv-python\",\"opencv-python-headless\",\n",
        "    \"av\",\"gradio\",\"gradio-client\",\"tokenizers\",\n",
        "    \"nvidia-cuda-runtime-cu12\",\"nvidia-cudnn-cu12\",\"nvidia-cublas-cu12\",\n",
        "    \"xformers\",\"jax\",\"jaxlib\"\n",
        ")\n",
        "\n",
        "# 1) Tooling\n",
        "pip(\"install\",\"-U\",\"pip\",\"setuptools\",\"wheel\")\n",
        "\n",
        "# 2) Foundation ABI pair (NUMPY 1.x!) + sklearn that supports Py3.12 and 1.x ABI\n",
        "pip(\"install\",\"--no-cache-dir\",\"--force-reinstall\",\"--no-deps\",\"numpy==1.26.4\")\n",
        "pip(\"install\",\"--no-cache-dir\",\"--force-reinstall\",\"--no-deps\",\"scipy==1.11.4\")\n",
        "pip(\"install\",\"--no-cache-dir\",\"--force-reinstall\",\"--no-deps\",\"scikit-learn==1.4.2\")\n",
        "\n",
        "# 3) LangGraph/LangChain pins (don’t touch numpy/torch)\n",
        "for dep in [\n",
        "    \"langgraph>=0.2.50,<0.3\",\n",
        "    \"langchain>=0.2.16,<0.4\",\n",
        "    \"langchain-core>=0.2.38,<0.4\",\n",
        "    \"langchain-community>=0.2.10,<0.4\",\n",
        "]:\n",
        "    pip(\"install\",\"--no-cache-dir\",\"--force-reinstall\",dep)\n",
        "\n",
        "# 4) PyTorch cu121 wheels (GPU)\n",
        "pip(\"install\",\"--no-cache-dir\",\"--force-reinstall\",\n",
        "    \"--index-url\",\"https://download.pytorch.org/whl/cu121\",\n",
        "    \"torch==2.3.0\",\"torchvision==0.18.0\",\"torchaudio==2.3.0\")\n",
        "\n",
        "# 5) Provide CUDA runtime libs (so libcudnn.so.8 etc. are present)\n",
        "pip(\"install\",\"--no-cache-dir\",\"--force-reinstall\",\n",
        "    \"nvidia-cuda-runtime-cu12==12.1.105\",\n",
        "    \"nvidia-cudnn-cu12==8.9.5.30\",\n",
        "    \"nvidia-cublas-cu12==12.1.3.1\"\n",
        ")\n",
        "\n",
        "# 6) HF stack (tokenizers BEFORE transformers) w/ minimal deps to avoid numpy upgrades\n",
        "pip(\"install\",\"--no-cache-dir\",\"--force-reinstall\",\"--no-deps\",\"tokenizers==0.21.0\")\n",
        "pip(\"install\",\"--no-cache-dir\",\"--force-reinstall\",\"--no-deps\",\"transformers==4.55.3\")\n",
        "pip(\"install\",\"--no-cache-dir\",\"--force-reinstall\",\"--no-deps\",\"accelerate==0.32.1\")\n",
        "pip(\"install\",\"--no-cache-dir\",\"--force-reinstall\",\"--no-deps\",\"huggingface_hub==0.34.4\")\n",
        "\n",
        "# 7) Other deps (no-deps so they cannot drag numpy/torch)\n",
        "for dep in [\n",
        "    \"gradio>=5.44.0\",\n",
        "    \"gradio-client>=1.4\",\n",
        "    \"GPUtil\",\"psutil\",\n",
        "    \"bitsandbytes==0.43.1\",  # requires CUDA; OK w/ GPU\n",
        "    \"av==12.0.0\",\n",
        "    \"opencv-python-headless==4.9.0.80\",\n",
        "    \"tiktoken==0.7.0\",\n",
        "    \"python-dotenv==1.0.1\",\n",
        "    \"mcp\",\n",
        "    \"packaging>=23.2\",\n",
        "]:\n",
        "    try:\n",
        "        pip(\"install\",\"--no-cache-dir\",\"--force-reinstall\",\"--no-deps\",dep)\n",
        "    except subprocess.CalledProcessError:\n",
        "        pip(\"install\",\"--no-cache-dir\",\"--force-reinstall\",dep)\n",
        "\n",
        "# 8) Belt & suspenders: re-pin the critical ones WITHOUT deps\n",
        "pip(\"install\",\"--no-cache-dir\",\"--force-reinstall\",\"--no-deps\",\"numpy==1.26.4\",\"scipy==1.11.4\",\"scikit-learn==1.4.2\")\n",
        "pip(\"install\",\"--no-cache-dir\",\"--force-reinstall\",\"--no-deps\",\"torch==2.3.0\",\"torchvision==0.18.0\",\"torchaudio==2.3.0\")\n",
        "\n",
        "# 9) sitecustomize: ensure CUDA libs on LD_LIBRARY_PATH every start\n",
        "nvidia_lib_dirs = []\n",
        "for root in site.getsitepackages():\n",
        "    for sub in [(\"nvidia\",\"cuda_runtime\",\"lib\"),(\"nvidia\",\"cudnn\",\"lib\"),(\"nvidia\",\"cublas\",\"lib\")]:\n",
        "        d = os.path.join(root,*sub)\n",
        "        if os.path.isdir(d): nvidia_lib_dirs.append(d)\n",
        "\n",
        "sc_path = os.path.join(site.getsitepackages()[0],\"sitecustomize.py\")\n",
        "with open(sc_path,\"w\") as f:\n",
        "    f.write(textwrap.dedent(f\"\"\"\n",
        "        import os\n",
        "        _paths = {nvidia_lib_dirs!r}\n",
        "        if _paths:\n",
        "            _cur = os.environ.get(\"LD_LIBRARY_PATH\",\"\")\n",
        "            _prefix = \":\".join(p for p in _paths if os.path.isdir(p))\n",
        "            if _prefix:\n",
        "                os.environ[\"LD_LIBRARY_PATH\"] = _prefix + (\":\" + _cur if _cur else \"\")\n",
        "            print(\"[sitecustomize] LD_LIBRARY_PATH set:\", os.environ.get(\"LD_LIBRARY_PATH\",\"\"))\n",
        "    \"\"\"))\n",
        "print(\"✅ Wrote\", sc_path)\n",
        "\n",
        "# 10) Restart so new wheels & LD_LIBRARY_PATH take effect for the next process\n",
        "import IPython; IPython.get_ipython().kernel.do_shutdown(restart=True)\n"
      ],
      "metadata": {
        "id": "9qgHqAFG_P61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bd6c506-31a6-458f-e9a8-736c9a55db3c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> pip uninstall -y torch torchvision torchaudio numpy scipy scikit-learn opencv-python opencv-python-headless av gradio gradio-client tokenizers nvidia-cuda-runtime-cu12 nvidia-cudnn-cu12 nvidia-cublas-cu12 xformers jax jaxlib\n",
            ">> pip install -U pip setuptools wheel\n",
            ">> pip install --no-cache-dir --force-reinstall --no-deps numpy==1.26.4\n",
            ">> pip install --no-cache-dir --force-reinstall --no-deps scipy==1.11.4\n",
            ">> pip install --no-cache-dir --force-reinstall --no-deps scikit-learn==1.4.2\n",
            ">> pip install --no-cache-dir --force-reinstall langgraph>=0.2.50,<0.3\n",
            ">> pip install --no-cache-dir --force-reinstall langchain>=0.2.16,<0.4\n",
            ">> pip install --no-cache-dir --force-reinstall langchain-core>=0.2.38,<0.4\n",
            ">> pip install --no-cache-dir --force-reinstall langchain-community>=0.2.10,<0.4\n",
            ">> pip install --no-cache-dir --force-reinstall --index-url https://download.pytorch.org/whl/cu121 torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0\n",
            ">> pip install --no-cache-dir --force-reinstall nvidia-cuda-runtime-cu12==12.1.105 nvidia-cudnn-cu12==8.9.5.30 nvidia-cublas-cu12==12.1.3.1\n",
            ">> pip install --no-cache-dir --force-reinstall --no-deps tokenizers==0.21.0\n",
            ">> pip install --no-cache-dir --force-reinstall --no-deps transformers==4.55.3\n",
            ">> pip install --no-cache-dir --force-reinstall --no-deps accelerate==0.32.1\n",
            ">> pip install --no-cache-dir --force-reinstall --no-deps huggingface_hub==0.34.4\n",
            ">> pip install --no-cache-dir --force-reinstall --no-deps gradio>=5.44.0\n",
            ">> pip install --no-cache-dir --force-reinstall --no-deps gradio-client>=1.4\n",
            ">> pip install --no-cache-dir --force-reinstall --no-deps GPUtil\n",
            ">> pip install --no-cache-dir --force-reinstall --no-deps psutil\n",
            ">> pip install --no-cache-dir --force-reinstall --no-deps bitsandbytes==0.43.1\n",
            ">> pip install --no-cache-dir --force-reinstall --no-deps av==12.0.0\n",
            ">> pip install --no-cache-dir --force-reinstall --no-deps opencv-python-headless==4.9.0.80\n",
            ">> pip install --no-cache-dir --force-reinstall --no-deps tiktoken==0.7.0\n",
            ">> pip install --no-cache-dir --force-reinstall --no-deps python-dotenv==1.0.1\n",
            ">> pip install --no-cache-dir --force-reinstall --no-deps mcp\n",
            ">> pip install --no-cache-dir --force-reinstall --no-deps packaging>=23.2\n",
            ">> pip install --no-cache-dir --force-reinstall --no-deps numpy==1.26.4 scipy==1.11.4 scikit-learn==1.4.2\n",
            ">> pip install --no-cache-dir --force-reinstall --no-deps torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0\n",
            "✅ Wrote /usr/local/lib/python3.12/dist-packages/sitecustomize.py\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib, numpy as np\n",
        "def v(m):\n",
        "    try: mobj = importlib.import_module(m); return getattr(mobj,\"__version__\",\"n/a\")\n",
        "    except Exception as e: return f\"ERR: {e}\"\n",
        "\n",
        "mods = [\"numpy\",\"scipy\",\"scikit_learn\",\"torch\",\"torchvision\",\"torchaudio\",\"transformers\",\"tokenizers\",\"cv2\",\"av\"]\n",
        "print(\"=== Versions ===\")\n",
        "for m in mods: print(f\"{m:14s}\", v(m))\n",
        "\n",
        "import torch\n",
        "print(\"\\nCUDA available:\", torch.cuda.is_available(), \"| CUDA:\", torch.version.cuda)\n",
        "if torch.cuda.is_available(): print(\"GPU:\", torch.cuda.get_device_name(0))\n"
      ],
      "metadata": {
        "id": "AghFFPjn4yVA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "535968f2-4057-41f9-ac62-13ed287f8846"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Versions ===\n",
            "numpy          1.26.4\n",
            "scipy          1.11.4\n",
            "scikit_learn   ERR: No module named 'scikit_learn'\n",
            "torch          2.3.0+cu121\n",
            "torchvision    0.18.0+cu121\n",
            "torchaudio     2.3.0+cu121\n",
            "transformers   4.55.3\n",
            "tokenizers     0.21.0\n",
            "cv2            4.9.0\n",
            "av             12.0.0\n",
            "\n",
            "CUDA available: True | CUDA: 12.1\n",
            "GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "import psutil\n",
        "import GPUtil\n",
        "\n",
        "def setup_gpu_environment():\n",
        "    \"\"\"Configure GPU environment for optimal performance\"\"\"\n",
        "\n",
        "    print(\"=== GPU Environment Setup ===\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"✅ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "        # Get GPU info\n",
        "        gpu_properties = torch.cuda.get_device_properties(0)\n",
        "        total_memory = gpu_properties.total_memory / 1024**3\n",
        "        print(f\"📊 Total GPU Memory: {total_memory:.1f} GB\")\n",
        "\n",
        "        # Set memory fraction (leave some room for system)\n",
        "        memory_fraction = 0.85 if total_memory > 15 else 0.75\n",
        "        torch.cuda.set_per_process_memory_fraction(memory_fraction)\n",
        "        print(f\"🎯 Memory Fraction Set: {memory_fraction}\")\n",
        "\n",
        "        # Clear any existing cache\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        return True, total_memory\n",
        "    else:\n",
        "        print(\"❌ No GPU available - will use CPU (very slow)\")\n",
        "        return False, 0\n",
        "\n",
        "# Check system resources\n",
        "def check_system_resources():\n",
        "    \"\"\"Check available system resources\"\"\"\n",
        "    print(\"\\n=== System Resources ===\")\n",
        "\n",
        "    # CPU info\n",
        "    print(f\"💻 CPU Cores: {psutil.cpu_count()}\")\n",
        "\n",
        "    # RAM info\n",
        "    ram = psutil.virtual_memory()\n",
        "    print(f\"🧠 RAM: {ram.total / 1024**3:.1f} GB total, {ram.available / 1024**3:.1f} GB available\")\n",
        "\n",
        "    # GPU info\n",
        "    try:\n",
        "        gpus = GPUtil.getGPUs()\n",
        "        if gpus:\n",
        "            for gpu in gpus:\n",
        "                print(f\"🎮 GPU: {gpu.name}, Memory: {gpu.memoryTotal} MB\")\n",
        "    except:\n",
        "        print(\"🎮 GPU info unavailable\")\n",
        "\n",
        "gpu_available, gpu_memory = setup_gpu_environment()\n",
        "check_system_resources()"
      ],
      "metadata": {
        "id": "fXlcEfYD3lUx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c1ff569-d23b-4b6a-96df-1e0c97b4c67f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== GPU Environment Setup ===\n",
            "✅ GPU Available: NVIDIA A100-SXM4-40GB\n",
            "📊 Total GPU Memory: 39.6 GB\n",
            "🎯 Memory Fraction Set: 0.85\n",
            "\n",
            "=== System Resources ===\n",
            "💻 CPU Cores: 12\n",
            "🧠 RAM: 83.5 GB total, 80.2 GB available\n",
            "🎮 GPU: NVIDIA A100-SXM4-40GB, Memory: 40960.0 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VideoLlava Agent with Proper MCP Integration\n",
        "# Improved LangGraph workflow, better error handling, and modular architecture\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import asyncio\n",
        "import logging\n",
        "from typing import Dict, List, Any, Optional, TypedDict, Annotated, Union\n",
        "from dataclasses import dataclass, field\n",
        "from enum import Enum\n",
        "import torch\n",
        "import av\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# LangGraph and LangChain imports\n",
        "from langgraph.graph import StateGraph, END, START\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "\n",
        "# VideoLlava imports\n",
        "from transformers import VideoLlavaProcessor, VideoLlavaForConditionalGeneration\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# MCP imports (proper MCP client implementation)\n",
        "try:\n",
        "    from mcp import ClientSession, StdioServerParameters\n",
        "    from mcp.client.stdio import stdio_client\n",
        "    MCP_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"Warning: MCP not installed. Install with: pip install mcp\")\n",
        "    MCP_AVAILABLE = False\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Configuration classes\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    \"\"\"Configuration for models\"\"\"\n",
        "    videollava_model: str = \"LanguageBind/Video-LLaVA-7B-hf\"\n",
        "    router_model: str = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    torch_dtype: torch.dtype = torch.float16\n",
        "    max_frames: int = 32\n",
        "    max_new_tokens: int = 256\n",
        "\n",
        "@dataclass\n",
        "class AgentConfig:\n",
        "    \"\"\"Configuration for agent behavior\"\"\"\n",
        "    enable_reasoning: bool = True\n",
        "    enable_memory: bool = True\n",
        "    max_tool_calls: int = 3\n",
        "    fallback_enabled: bool = True\n",
        "    debug_mode: bool = False\n",
        "\n",
        "class ToolType(Enum):\n",
        "    \"\"\"Enumeration of available tool types\"\"\"\n",
        "    VISUAL_ANALYSIS = \"visual_analysis\"\n",
        "    METADATA_EXTRACTION = \"metadata_extraction\"\n",
        "    COMPREHENSIVE_SUMMARY = \"comprehensive_summary\"\n",
        "    ANOMALY_DETECTION = \"anomaly_detection\"\n",
        "    STEP_BY_STEP = \"step_by_step\"\n",
        "    WEB_SEARCH = \"web_search\"\n",
        "    RESPOND = \"respond\"\n",
        "    LIST_TOOLS = \"list_tools\"\n",
        "\n",
        "# Agent State\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"state for VideoLlava agent with better type hints\"\"\"\n",
        "    messages: Annotated[List[BaseMessage], \"Chat messages\"]\n",
        "    video_path: Optional[str]\n",
        "    current_task: Optional[str]\n",
        "    tool_calls: List[Dict[str, Any]]\n",
        "    context: Dict[str, Any]\n",
        "    llm_classification: Optional[Dict[str, str]]\n",
        "    error_context: Optional[Dict[str, str]]\n",
        "    session_id: str\n",
        "    reasoning_trace: List[str]\n",
        "\n",
        "# Video processing utilities\n",
        "class VideoProcessor:\n",
        "    \"\"\"Video processing utilities\"\"\"\n",
        "    @staticmethod\n",
        "    def uniform_sample_frames(video_path: str, num_frames: int = 32):\n",
        "        \"\"\"Uniformly sample frames across the whole video; returns list[PIL.Image]\"\"\"\n",
        "        try:\n",
        "            container = av.open(video_path)\n",
        "            stream = container.streams.video[0]\n",
        "            total = stream.frames or 0\n",
        "\n",
        "            # Fall back: if no frame count, just decode and sample by index\n",
        "            indices = (np.linspace(0, max(total - 1, 0), num_frames).astype(int).tolist()\n",
        "                      if total > 0 else None)\n",
        "\n",
        "            images = []\n",
        "            i = 0\n",
        "            target_set = set(indices) if indices is not None else None\n",
        "\n",
        "            for frame in container.decode(video=0):\n",
        "                take = (target_set is None) or (i in target_set)\n",
        "                if take:\n",
        "                    img = frame.to_ndarray(format=\"rgb24\")\n",
        "                    images.append(Image.fromarray(img))\n",
        "                    if len(images) >= num_frames:\n",
        "                        break\n",
        "                i += 1\n",
        "\n",
        "            container.close()\n",
        "            return images\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Uniform sampling error: {e}\")\n",
        "            return []\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_keyframes(video_path: str, max_frames: int = 32, threshold: float = 0.3):\n",
        "        \"\"\"Extract keyframes as a list of PIL.Image frames\"\"\"\n",
        "        try:\n",
        "            import cv2\n",
        "            container = av.open(video_path)\n",
        "            prev_hist = None\n",
        "            selected = []\n",
        "\n",
        "            for frame in container.decode(video=0):\n",
        "                img = frame.to_ndarray(format=\"rgb24\")        # (H, W, 3), RGB\n",
        "                img_small = cv2.resize(img, (160, 90))\n",
        "                hist = cv2.calcHist([img_small], [0, 1, 2], None,\n",
        "                                    [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
        "                hist = cv2.normalize(hist, hist).flatten()\n",
        "\n",
        "                if prev_hist is None or cv2.compareHist(prev_hist, hist, cv2.HISTCMP_BHATTACHARYYA) > threshold:\n",
        "                    selected.append(Image.fromarray(img))      # convert to PIL here\n",
        "                    prev_hist = hist\n",
        "\n",
        "                if len(selected) >= max_frames:\n",
        "                    break\n",
        "\n",
        "            container.close()\n",
        "            return selected  # <-- list[ PIL.Image ]\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting keyframes: {e}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def get_video_metadata(video_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract comprehensive video metadata\"\"\"\n",
        "        try:\n",
        "            container = av.open(video_path)\n",
        "            video_stream = container.streams.video[0]\n",
        "\n",
        "            # CORRECT conversion: AV_TIME_BASE units -> seconds\n",
        "            duration_seconds = float(container.duration * av.time_base) if container.duration is not None else 0.0\n",
        "\n",
        "            # fps fallback\n",
        "            if video_stream.average_rate:\n",
        "                fps = float(video_stream.average_rate)\n",
        "            elif getattr(video_stream, \"base_rate\", None):\n",
        "                fps = float(video_stream.base_rate)\n",
        "            else:\n",
        "                fps = 0.0\n",
        "\n",
        "            metadata = {\n",
        "                \"duration_seconds\": duration_seconds,\n",
        "                \"fps\": fps,\n",
        "                \"frames\": video_stream.frames,\n",
        "                \"width\": video_stream.width,\n",
        "                \"height\": video_stream.height,\n",
        "                \"codec\": video_stream.codec_context.name,\n",
        "                \"bit_rate\": video_stream.bit_rate,\n",
        "                \"pixel_format\": str(video_stream.pix_fmt)\n",
        "            }\n",
        "\n",
        "            container.close()\n",
        "            return metadata\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting metadata: {e}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "# LLM Router with better error handling\n",
        "class LLMRouter:\n",
        "    \"\"\"Improved LLM router with better classification and error handling\"\"\"\n",
        "\n",
        "    def __init__(self, model_config: ModelConfig):\n",
        "        self.model_config = model_config\n",
        "        self.device = model_config.device\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "        self._initialize_model()\n",
        "\n",
        "    def _initialize_model(self):\n",
        "        \"\"\"Initialize routing model with comprehensive error handling\"\"\"\n",
        "        try:\n",
        "            logger.info(f\"Loading routing model: {self.model_config.router_model}\")\n",
        "\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                self.model_config.router_model,\n",
        "                trust_remote_code=True,\n",
        "                padding_side=\"left\"\n",
        "            )\n",
        "\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.model_config.router_model,\n",
        "                torch_dtype=self.model_config.torch_dtype,\n",
        "                device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "                trust_remote_code=True,\n",
        "                low_cpu_mem_usage=True,\n",
        "                use_cache=False,\n",
        "                attn_implementation=\"eager\"\n",
        "            )\n",
        "\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "            self.model.eval()\n",
        "            logger.info(\"LLM Router initialized successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to initialize LLM Router: {e}\")\n",
        "            self.model = None\n",
        "            self.tokenizer = None\n",
        "\n",
        "    def classify_request(self, user_question: str, has_video: bool = True) -> Dict[str, str]:\n",
        "        \"\"\"classification with structured reasoning\"\"\"\n",
        "        content = user_question.lower()\n",
        "\n",
        "        def pack(tool: str, reason: str, conf=\"0.8\"):\n",
        "            return {\n",
        "                \"primary_tool\": tool,\n",
        "                \"reasoning\": reason,\n",
        "                \"custom_prompt\": user_question,\n",
        "                \"confidence\": conf,\n",
        "            }\n",
        "\n",
        "        if not has_video:\n",
        "            # Route by intent even without video\n",
        "            if any(k in content for k in [\"duration\", \"fps\", \"frame rate\", \"resolution\", \"codec\", \"bitrate\", \"metadata\"]):\n",
        "                return pack(\"metadata_extraction\", \"Tech question without video; tool will handle missing video.\")\n",
        "            if any(k in content for k in [\"step\", \"chronolog\", \"sequence\", \"timeline\"]):\n",
        "                return pack(\"step_by_step\", \"Sequential request without video; tool will handle missing video.\")\n",
        "            if any(k in content for k in [\"unusual\", \"strange\", \"weird\", \"odd\", \"unexpected\", \"anomal\"]):\n",
        "                return pack(\"anomaly_detection\", \"Anomaly request without video; tool will handle missing video.\")\n",
        "            if any(k in content for k in [\"tools\", \"capabilities\", \"help\", \"what can\"]):\n",
        "                return pack(\"list_tools\", \"User asked about capabilities.\")\n",
        "            # Default to visual_analysis so we try to answer from text context (if any)\n",
        "            return pack(\"visual_analysis\", \"No video provided; attempt text-based answer if possible.\", \"0.7\")\n",
        "\n",
        "        try:\n",
        "            prompt = self._create_classification_prompt(user_question, has_video)\n",
        "\n",
        "            inputs = self.tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=1024,\n",
        "                padding=True\n",
        "            )\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=200,\n",
        "                    do_sample=False,\n",
        "                    pad_token_id=self.tokenizer.eos_token_id,\n",
        "                    eos_token_id=self.tokenizer.eos_token_id,\n",
        "                    use_cache=False\n",
        "                )\n",
        "\n",
        "            response = self.tokenizer.decode(\n",
        "                outputs[0][inputs['input_ids'].shape[1]:],\n",
        "                skip_special_tokens=True\n",
        "            ).strip()\n",
        "\n",
        "            return self._parse_classification_response(response, user_question, has_video)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"LLM classification failed: {e}, using fallback\")\n",
        "            return self._fallback_classification(user_question, has_video)\n",
        "\n",
        "    def _create_classification_prompt(self, user_question: str, has_video: bool) -> str:\n",
        "        \"\"\"Create structured classification prompt\"\"\"\n",
        "        video_status = \"Available\" if has_video else \"Not available\"\n",
        "\n",
        "        return f\"\"\"Analyze this user question and classify it for a video analysis system.\n",
        "\n",
        "Question: \"{user_question}\"\n",
        "Video: {video_status}\n",
        "\n",
        "Available tools:\n",
        "1. visual_analysis - Describe content, count objects, identify elements\n",
        "2. metadata_extraction - Technical specs (duration, fps, resolution)\n",
        "3. comprehensive_summary - Multi-angle analysis\n",
        "4. anomaly_detection - Find unusual elements\n",
        "5. step_by_step - Chronological breakdown\n",
        "6. web_search - External information lookup\n",
        "7. list_tools - Show available capabilities\n",
        "8. respond - General conversation\n",
        "\n",
        "Classification rules:\n",
        "- Technical questions → metadata_extraction\n",
        "- Visual/content questions → visual_analysis\n",
        "- Comprehensive requests → comprehensive_summary\n",
        "- Unusual/anomaly requests → anomaly_detection\n",
        "- Sequential requests → step_by_step\n",
        "- Tool inquiries → list_tools\n",
        "- No video + video questions → respond\n",
        "\n",
        "Respond in JSON format:\n",
        "{{\"tool\": \"tool_name\", \"confidence\": 0.9, \"reasoning\": \"explanation\"}}\"\"\"\n",
        "\n",
        "    def _parse_classification_response(self, response: str, user_question: str, has_video: bool) -> Dict[str, str]:\n",
        "        \"\"\"Parse LLM classification response\"\"\"\n",
        "        try:\n",
        "            # Extract JSON from response\n",
        "            start_idx = response.find('{')\n",
        "            end_idx = response.rfind('}') + 1\n",
        "\n",
        "            if start_idx != -1 and end_idx > start_idx:\n",
        "                json_str = response[start_idx:end_idx]\n",
        "                classification = json.loads(json_str)\n",
        "\n",
        "                return {\n",
        "                    \"primary_tool\": classification.get(\"tool\", \"visual_analysis\"),\n",
        "                    \"confidence\": str(classification.get(\"confidence\", 0.8)),\n",
        "                    \"reasoning\": classification.get(\"reasoning\", \"LLM classification\"),\n",
        "                    \"custom_prompt\": user_question\n",
        "                }\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return self._fallback_classification(user_question, has_video)\n",
        "\n",
        "    def _fallback_classification(self, user_question: str, has_video: bool) -> Dict[str, str]:\n",
        "        \"\"\"fallback classification with pattern matching\"\"\"\n",
        "        content = user_question.lower()\n",
        "\n",
        "        if not has_video:\n",
        "            def pack(tool, reason, conf=\"0.8\"):\n",
        "                return {\"primary_tool\": tool, \"reasoning\": reason, \"custom_prompt\": user_question, \"confidence\": conf}\n",
        "            if any(k in content for k in [\"duration\",\"fps\",\"frame rate\",\"resolution\",\"codec\",\"bitrate\",\"metadata\"]):\n",
        "                return pack(\"metadata_extraction\", \"Tech question without video; tool will handle missing video.\")\n",
        "            if any(k in content for k in [\"step\",\"chronolog\",\"sequence\",\"timeline\"]):\n",
        "                return pack(\"step_by_step\", \"Sequential request without video; tool will handle missing video.\")\n",
        "            if any(k in content for k in [\"unusual\",\"strange\",\"weird\",\"odd\",\"unexpected\",\"anomal\"]):\n",
        "                return pack(\"anomaly_detection\", \"Anomaly request without video; tool will handle missing video.\")\n",
        "            if any(k in content for k in [\"tools\",\"capabilities\",\"help\",\"what can\"]):\n",
        "                return pack(\"list_tools\", \"User asked about capabilities.\")\n",
        "            return pack(\"visual_analysis\", \"No video provided; attempt text-based answer if possible.\", \"0.7\")\n",
        "\n",
        "        # pattern matching\n",
        "        patterns = {\n",
        "            \"metadata_extraction\": [\"duration\", \"long\", \"fps\", \"frame\", \"resolution\", \"specs\", \"technical\", \"codec\"],\n",
        "            \"comprehensive_summary\": [\"comprehensive\", \"full\", \"everything\", \"complete\", \"detailed\"],\n",
        "            \"anomaly_detection\": [\"unusual\", \"strange\", \"weird\", \"anomal\", \"odd\", \"unexpected\"],\n",
        "            \"step_by_step\": [\"step\", \"break down\", \"chronological\", \"sequence\", \"timeline\"],\n",
        "            \"list_tools\": [\"tools\", \"available\", \"capabilities\", \"help\", \"what can\"],\n",
        "            \"visual_analysis\": [\"what\", \"describe\", \"see\", \"happening\", \"count\", \"identify\"]\n",
        "        }\n",
        "\n",
        "        for tool, keywords in patterns.items():\n",
        "            if any(keyword in content for keyword in keywords):\n",
        "                return {\n",
        "                    \"primary_tool\": tool,\n",
        "                    \"reasoning\": f\"Pattern match: {tool}\",\n",
        "                    \"custom_prompt\": user_question,\n",
        "                    \"confidence\": \"0.7\"\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            \"primary_tool\": \"visual_analysis\",\n",
        "            \"reasoning\": \"Default fallback\",\n",
        "            \"custom_prompt\": user_question,\n",
        "            \"confidence\": \"0.5\"\n",
        "        }\n",
        "\n",
        "# MCP Tool Integration\n",
        "class MCPToolManager:\n",
        "    \"\"\"Manager for MCP (Model Context Protocol) tools\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.sessions = {}\n",
        "        self.available_servers = []\n",
        "        self.mcp_enabled = MCP_AVAILABLE\n",
        "\n",
        "    async def initialize_mcp_server(self, server_name: str, command: List[str]) -> bool:\n",
        "        \"\"\"Initialize an MCP server connection\"\"\"\n",
        "        if not self.mcp_enabled:\n",
        "            logger.warning(\"MCP not available\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            server_params = StdioServerParameters(\n",
        "                command=command,\n",
        "                env=None\n",
        "            )\n",
        "\n",
        "            session = await stdio_client(server_params).__aenter__()\n",
        "            self.sessions[server_name] = session\n",
        "\n",
        "            # Initialize the session\n",
        "            init_result = await session.initialize()\n",
        "            logger.info(f\"MCP server {server_name} initialized: {init_result}\")\n",
        "\n",
        "            # List available tools\n",
        "            tools_result = await session.list_tools()\n",
        "            logger.info(f\"Available tools from {server_name}: {[tool.name for tool in tools_result.tools]}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to initialize MCP server {server_name}: {e}\")\n",
        "            return False\n",
        "\n",
        "    async def call_mcp_tool(self, server_name: str, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Call an MCP tool\"\"\"\n",
        "        if server_name not in self.sessions:\n",
        "            raise ValueError(f\"MCP server {server_name} not initialized\")\n",
        "\n",
        "        try:\n",
        "            session = self.sessions[server_name]\n",
        "            result = await session.call_tool(tool_name, arguments)\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            logger.error(f\"MCP tool call failed: {e}\")\n",
        "            raise\n",
        "\n",
        "# Video Analysis Tools\n",
        "class VideoAnalysisTools:\n",
        "    \"\"\"Collection of video analysis tools\"\"\"\n",
        "\n",
        "    def __init__(self, model_config: ModelConfig, mcp_manager: MCPToolManager):\n",
        "        self.model_config = model_config\n",
        "        self.mcp_manager = mcp_manager\n",
        "        self.processor = None\n",
        "        self.model = None\n",
        "        self._initialize_models()\n",
        "        self._frame_cache = {}\n",
        "\n",
        "    import re as _re\n",
        "\n",
        "    def _extract_count(self, text: str) -> str:\n",
        "        \"\"\"Return a strict integer or short range from model text.\"\"\"\n",
        "        t = text.strip()\n",
        "        # exact integer\n",
        "        m = _re.search(r\"\\b(\\d+)\\b\", t)\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "        # short range like 2-3 or 2–3\n",
        "        m = _re.search(r\"\\b(\\d+)\\s*[-–]\\s*(\\d+)\\b\", t)\n",
        "        if m:\n",
        "            a, b = int(m.group(1)), int(m.group(2))\n",
        "            return f\"{a}-{b}\" if a <= b else f\"{b}-{a}\"\n",
        "        # words fallback\n",
        "        words = {\"one\":\"1\",\"two\":\"2\",\"three\":\"3\",\"four\":\"4\",\"five\":\"5\",\"six\":\"6\",\"seven\":\"7\",\"eight\":\"8\",\"nine\":\"9\",\"ten\":\"10\"}\n",
        "        for w,n in words.items():\n",
        "            if _re.search(rf\"\\b{w}\\b\", t.lower()):\n",
        "                return n\n",
        "        return t\n",
        "\n",
        "    def _initialize_models(self):\n",
        "        \"\"\"Initialize VideoLlava models\"\"\"\n",
        "        try:\n",
        "            logger.info(f\"Loading VideoLlava: {self.model_config.videollava_model}\")\n",
        "\n",
        "            self.processor = VideoLlavaProcessor.from_pretrained(self.model_config.videollava_model)\n",
        "            self.model = VideoLlavaForConditionalGeneration.from_pretrained(\n",
        "                self.model_config.videollava_model,\n",
        "                torch_dtype=self.model_config.torch_dtype,\n",
        "                device_map=\"auto\",\n",
        "                low_cpu_mem_usage=True\n",
        "            )\n",
        "\n",
        "            logger.info(\"VideoLlava models loaded successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load VideoLlava: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _get_reference_frames(self, video_path: str) -> List[Image.Image]:\n",
        "        \"\"\"Cache keyframes once per video for consistent answers across turns.\"\"\"\n",
        "        if video_path in self._frame_cache:\n",
        "            return self._frame_cache[video_path]\n",
        "        frames = VideoProcessor.extract_keyframes(video_path, max_frames=self.model_config.max_frames)\n",
        "        self._frame_cache[video_path] = frames\n",
        "        return frames\n",
        "\n",
        "    def _build_chat_prompt(self, question: str, preface: str = \"\") -> str:\n",
        "        \"\"\"\n",
        "        Video-LLaVA prefers chat-style prompts. Always include exactly one <video>.\n",
        "        \"\"\"\n",
        "        system = \"You are a helpful assistant that watches the video and answers concisely and factually.\"\n",
        "        pre = (preface.strip() + \"\\n\") if preface else \"\"\n",
        "        return f\"{system}\\nUSER: <video>\\n{pre}{question}\\nASSISTANT:\"\n",
        "\n",
        "    def _build_prompt_and_frames(self, video_path: str, user_q: str):\n",
        "        \"\"\"\n",
        "        Decide which frames to use and return a brief 'preface' for the prompt.\n",
        "        Returns (frames: List[PIL.Image], preface: str, mode: str)\n",
        "        mode in {\"count\",\"timeline\",\"default\",\"tech\"}\n",
        "        \"\"\"\n",
        "        q = (user_q or \"\").strip()\n",
        "        ql = q.lower()\n",
        "\n",
        "        # Expanded intent detection\n",
        "        is_timeline = bool(re.search(r\"\\b(step[- ]?by[- ]?step|timeline|sequence|chronolog|beginning|middle|end|conclude|conclusion)\\b\", ql))\n",
        "        is_count    = bool(re.search(r\"\\b(how many|count|number of)\\b\", ql))\n",
        "        is_tech     = bool(re.search(r\"\\b(duration|fps|frame rate|resolution|codec|bit[- ]?rate|metadata)\\b\", ql))\n",
        "\n",
        "        if is_tech:\n",
        "            return None, \"__TECH__\", \"tech\"\n",
        "\n",
        "        if is_timeline:\n",
        "            # For timeline, use uniform sampling to preserve order\n",
        "            frames = VideoProcessor.uniform_sample_frames(\n",
        "                video_path, num_frames=min(48, self.model_config.max_frames * 2)\n",
        "            )\n",
        "            preface = (\n",
        "                \"Provide a concise step-by-step timeline of the video.\\n\"\n",
        "                \"• Use 5–8 bullet points, one sentence each.\\n\"\n",
        "                \"• If timing is inferable, prefix with [MM:SS]; otherwise omit.\\n\"\n",
        "                \"• Be specific about actions; avoid generic descriptions.\"\n",
        "            )\n",
        "            return frames, preface, \"timeline\"\n",
        "\n",
        "        # For all other tasks, use the cached reference pack for consistency\n",
        "        frames = self._get_reference_frames(video_path)\n",
        "\n",
        "        if is_count:\n",
        "            preface = (\n",
        "                \"Count the number of distinct people visible in the video frames.\\n\"\n",
        "                \"Return ONLY a single integer; if uncertain, return a short range like '2-3'.\\n\"\n",
        "                \"Do NOT add words or explanations.\"\n",
        "            )\n",
        "            return frames, preface, \"count\"\n",
        "\n",
        "        preface = (\n",
        "            \"Answer the user's question based ONLY on the video frames.\\n\"\n",
        "            \"Be concise and specific. If information is not visible, say 'not visible'.\\n\"\n",
        "            \"Do NOT provide a generic caption or unrelated details.\"\n",
        "        )\n",
        "        return frames, preface, \"default\"\n",
        "\n",
        "\n",
        "\n",
        "    def analyze_video_content(self, video_path: str, user_q: str) -> str:\n",
        "        try:\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            frames, preface, mode = self._build_prompt_and_frames(video_path, user_q)\n",
        "\n",
        "            if mode == \"tech\":\n",
        "                return self.extract_metadata(video_path)\n",
        "\n",
        "            if not frames:\n",
        "                return \"Error analyzing video: no frames extracted (unsupported/empty video).\"\n",
        "\n",
        "            prompt = self._build_chat_prompt(question=user_q, preface=preface)\n",
        "\n",
        "            inputs = self.processor(text=prompt, videos=[frames], return_tensors=\"pt\")\n",
        "            if torch.cuda.is_available():\n",
        "                inputs = {k: (v.to(self.model_config.device) if isinstance(v, torch.Tensor) else v)\n",
        "                          for k, v in inputs.items()}\n",
        "\n",
        "            gen_kwargs = dict(\n",
        "                max_new_tokens=max(96, self.model_config.max_new_tokens),\n",
        "                pad_token_id=self.processor.tokenizer.eos_token_id,\n",
        "                eos_token_id=self.processor.tokenizer.eos_token_id,\n",
        "                use_cache=True,\n",
        "            )\n",
        "\n",
        "            if mode == \"count\":\n",
        "                # Deterministic, short output\n",
        "                gen_kwargs.update(dict(do_sample=False, temperature=0.0, top_p=1.0, repetition_penalty=1.0, max_new_tokens=8))\n",
        "            else:\n",
        "                # Gentle sampling to avoid empty outputs\n",
        "                gen_kwargs.update(dict(do_sample=True, temperature=0.2, top_p=0.9, repetition_penalty=1.05))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                ids = self.model.generate(**inputs, **gen_kwargs)\n",
        "\n",
        "            # Decode only continuation\n",
        "            if \"input_ids\" in inputs:\n",
        "                gen_only = ids[0, inputs[\"input_ids\"].shape[1]:]\n",
        "            else:\n",
        "                gen_only = ids[0]\n",
        "            text = self.processor.batch_decode(gen_only.unsqueeze(0), skip_special_tokens=True, clean_up_tokenization_spaces=False)[0].strip()\n",
        "\n",
        "            if mode == \"count\":\n",
        "                return self._extract_count(text)\n",
        "            return text if text else \"No answer produced.\"\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Video analysis error: {e}\")\n",
        "            return f\"Error analyzing video: {str(e)}\"\n",
        "\n",
        "\n",
        "    def extract_metadata(self, video_path: str) -> str:\n",
        "        \"\"\"Extract comprehensive video metadata\"\"\"\n",
        "        try:\n",
        "            metadata = VideoProcessor.get_video_metadata(video_path)\n",
        "\n",
        "            # Format duration\n",
        "            duration = metadata['duration_seconds']\n",
        "            if duration > 0:\n",
        "                minutes = int(duration // 60)\n",
        "                seconds = duration % 60\n",
        "                duration_str = f\"{minutes}m {seconds:.1f}s\" if minutes > 0 else f\"{seconds:.1f}s\"\n",
        "            else:\n",
        "                duration_str = \"Unknown\"\n",
        "\n",
        "            # Create formatted output\n",
        "            output = f\"\"\"\n",
        "**Video Metadata Analysis**\n",
        "\n",
        "**Duration**: {duration_str} ({duration:.2f} seconds)\n",
        "**Frame Count**: {metadata['frames']} frames\n",
        "**Frame Rate**: {metadata['fps']:.2f} fps\n",
        "**Resolution**: {metadata['width']}x{metadata['height']}\n",
        "**Codec**: {metadata['codec']}\n",
        "**Bit Rate**: {metadata.get('bit_rate', 'Unknown')}\n",
        "**Pixel Format**: {metadata.get('pixel_format', 'Unknown')}\n",
        "\n",
        "**Technical Summary**: This is a {duration_str} video with {metadata['frames']} frames at {metadata['fps']:.1f} fps, recorded in {metadata['width']}x{metadata['height']} resolution using {metadata['codec']} codec.\n",
        "            \"\"\".strip()\n",
        "\n",
        "            return output\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Metadata extraction error: {e}\")\n",
        "            return f\"Error extracting metadata: {str(e)}\"\n",
        "\n",
        "# Agent Nodes\n",
        "class AgentNodes:\n",
        "    \"\"\"agent nodes with better error handling and reasoning\"\"\"\n",
        "\n",
        "    def __init__(self, tools: VideoAnalysisTools, router: LLMRouter, config: AgentConfig):\n",
        "        self.tools = tools\n",
        "        self.router = router\n",
        "        self.config = config\n",
        "\n",
        "    def route_request(self, state: AgentState) -> str:\n",
        "        \"\"\"routing with better decision making\"\"\"\n",
        "        try:\n",
        "            last_message = state[\"messages\"][-1]\n",
        "\n",
        "            if not isinstance(last_message, HumanMessage):\n",
        "                return \"respond\"\n",
        "\n",
        "            has_video = bool(state.get(\"video_path\"))\n",
        "            classification = self.router.classify_request(last_message.content, has_video)\n",
        "\n",
        "            # Store classification for later use\n",
        "            state[\"llm_classification\"] = classification\n",
        "\n",
        "            # Add reasoning trace\n",
        "            if self.config.enable_reasoning:\n",
        "                reasoning = f\"Router decision: {classification['primary_tool']} - {classification['reasoning']}\"\n",
        "                state.setdefault(\"reasoning_trace\", []).append(reasoning)\n",
        "\n",
        "            # Map tools to routes\n",
        "            tool_routes = {\n",
        "                \"visual_analysis\": \"analyze_video\",\n",
        "                \"metadata_extraction\": \"analyze_video\",\n",
        "                \"comprehensive_summary\": \"analyze_video\",\n",
        "                \"anomaly_detection\": \"analyze_video\",\n",
        "                \"step_by_step\": \"analyze_video\",\n",
        "                \"web_search\": \"analyze_video\",\n",
        "                \"list_tools\": \"list_tools\",\n",
        "                \"respond\": \"respond\"\n",
        "            }\n",
        "\n",
        "            route = tool_routes.get(classification[\"primary_tool\"], \"respond\")\n",
        "\n",
        "            if self.config.debug_mode:\n",
        "                logger.info(f\"Routing decision: {route} (tool: {classification['primary_tool']})\")\n",
        "\n",
        "            return route\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Routing error: {e}\")\n",
        "            state[\"error_context\"] = {\"error\": str(e), \"stage\": \"routing\"}\n",
        "            return \"respond\"\n",
        "\n",
        "    def analyze_video_node(self, state: AgentState) -> AgentState:\n",
        "        \"\"\"video analysis node with tool execution\"\"\"\n",
        "        try:\n",
        "            video_path = state.get(\"video_path\")\n",
        "            if not video_path:\n",
        "                return {\n",
        "                    **state,\n",
        "                    \"messages\": [AIMessage(content=\"Please upload a video first.\")]\n",
        "                }\n",
        "\n",
        "            classification = state.get(\"llm_classification\") or {}\n",
        "            tool_type = classification.get(\"primary_tool\", \"visual_analysis\")\n",
        "            custom_prompt = classification.get(\"custom_prompt\", \"Describe this video.\")\n",
        "\n",
        "            # Execute appropriate analysis\n",
        "            if tool_type == \"metadata_extraction\":\n",
        "                result = self.tools.extract_metadata(video_path)\n",
        "            elif tool_type == \"comprehensive_summary\":\n",
        "                result = self._comprehensive_analysis(video_path)\n",
        "            elif tool_type == \"anomaly_detection\":\n",
        "                result = self.tools.analyze_video_content(\n",
        "                    video_path,\n",
        "                    \"Identify any unusual, strange, or unexpected elements in this video.\"\n",
        "                )\n",
        "            elif tool_type == \"step_by_step\":\n",
        "                result = self._step_by_step_analysis(video_path)\n",
        "            else:  # visual_analysis or default\n",
        "                result = self.tools.analyze_video_content(video_path, custom_prompt)\n",
        "\n",
        "            # Add reasoning if enabled\n",
        "            if self.config.enable_reasoning and classification.get(\"reasoning\"):\n",
        "                result += f\"\\n\\n**Analysis Method**: {classification['reasoning']}\"\n",
        "\n",
        "            return {\n",
        "                **state,\n",
        "                \"messages\": [AIMessage(content=result)],\n",
        "                \"tool_calls\": [{\"tool\": tool_type, \"result\": \"success\"}]\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Video analysis error: {e}\")\n",
        "            error_msg = f\"Error during video analysis: {str(e)}\"\n",
        "            return {\n",
        "                **state,\n",
        "                \"messages\": [AIMessage(content=error_msg)],\n",
        "                \"error_context\": {\"error\": str(e), \"stage\": \"video_analysis\"}\n",
        "            }\n",
        "\n",
        "    def _comprehensive_analysis(self, video_path: str) -> str:\n",
        "        \"\"\"Perform comprehensive video analysis with multiple passes\"\"\"\n",
        "        analyses = []\n",
        "\n",
        "        prompts = [\n",
        "            \"What is the main subject or content of this video?\",\n",
        "            \"Describe the visual elements, setting, and environment.\",\n",
        "            \"What actions or activities are taking place?\",\n",
        "            \"What is the overall mood or atmosphere?\",\n",
        "            \"Provide any additional notable observations.\"\n",
        "        ]\n",
        "\n",
        "        for i, prompt in enumerate(prompts, 1):\n",
        "            try:\n",
        "                result = self.tools.analyze_video_content(video_path, prompt)\n",
        "                analyses.append(f\"**Analysis {i}**: {result}\")\n",
        "            except Exception as e:\n",
        "                analyses.append(f\"**Analysis {i}**: Error - {str(e)}\")\n",
        "\n",
        "        return \"**Comprehensive Video Analysis**\\n\\n\" + \"\\n\\n\".join(analyses)\n",
        "\n",
        "    # def _step_by_step_analysis(self, video_path: str) -> str:\n",
        "    #     \"\"\"Perform step-by-step chronological analysis\"\"\"\n",
        "    #     analyses = []\n",
        "\n",
        "    #     prompts = [\n",
        "    #         \"Describe what happens at the beginning of this video.\",\n",
        "    #         \"What occurs in the middle section of the video?\",\n",
        "    #         \"How does the video conclude?\",\n",
        "    #         \"Summarize the overall sequence of events.\"\n",
        "    #     ]\n",
        "\n",
        "    #     for i, prompt in enumerate(prompts, 1):\n",
        "    #         try:\n",
        "    #             result = self.tools.analyze_video_content(video_path, prompt)\n",
        "    #             analyses.append(f\"**Step {i}**: {result}\")\n",
        "    #         except Exception as e:\n",
        "    #             analyses.append(f\"**Step {i}**: Error - {str(e)}\")\n",
        "\n",
        "    #     return \"**Step-by-Step Video Analysis**\\n\\n\" + \"\\n\\n\".join(analyses)\n",
        "\n",
        "    def _step_by_step_analysis(self, video_path: str) -> str:\n",
        "        \"\"\"Single-pass timeline using uniform sampling for ordered events.\"\"\"\n",
        "        q = \"Give me a concise step-by-step timeline of what happens in the video.\"\n",
        "        return self.analyze_video_content(video_path, q)\n",
        "\n",
        "\n",
        "    def list_tools_node(self, state: AgentState) -> AgentState:\n",
        "        \"\"\"tools listing with capabilities\"\"\"\n",
        "        tools_info = \"\"\"\n",
        "**VideoLlava Agent Capabilities**\n",
        "\n",
        "**Core Analysis Tools:**\n",
        "- **Visual Analysis**: Describe content, count objects, identify elements\n",
        "- **Metadata Extraction**: Technical specifications (duration, fps, resolution)\n",
        "- **Comprehensive Summary**: Multi-angle analysis with multiple perspectives\n",
        "\n",
        "**Advanced Analysis:**\n",
        "- **Anomaly Detection**: Identify unusual or unexpected elements\n",
        "- **Step-by-Step Analysis**: Chronological breakdown of video events\n",
        "- **Web Search**: External context and information lookup (when enabled)\n",
        "\n",
        "**System Features:**\n",
        "- **Intelligent Routing**: LLM-powered tool selection\n",
        "- **Memory Persistence**: Video remains loaded between questions\n",
        "- **Error Recovery**: Robust fallback mechanisms\n",
        "- **Reasoning Traces**: See why each tool was selected\n",
        "\n",
        "**How to Use:**\n",
        "Simply upload a video and ask natural questions. The system will automatically select the most appropriate analysis method.\n",
        "\n",
        "Ready to analyze your video!\n",
        "        \"\"\".strip()\n",
        "\n",
        "        return {\n",
        "            **state,\n",
        "            \"messages\": [AIMessage(content=tools_info)]\n",
        "        }\n",
        "\n",
        "    def respond_node(self, state: AgentState) -> AgentState:\n",
        "        \"\"\"response node with context awareness (uses router LLM if available)\"\"\"\n",
        "        last_msg = state[\"messages\"][-1] if state.get(\"messages\") else None\n",
        "        question = getattr(last_msg, \"content\", \"\") if last_msg else \"\"\n",
        "        error_context = state.get(\"error_context\")\n",
        "\n",
        "        # If we have an error, keep your troubleshooting block\n",
        "        if error_context:\n",
        "            response = (\n",
        "                f\"I encountered an issue: {error_context['error']}\\n\\n\"\n",
        "                \"**Troubleshooting Steps:**\\n\"\n",
        "                \"1. Ensure your video file is properly uploaded\\n\"\n",
        "                \"2. Check that the video format is supported (MP4, AVI, MOV, etc.)\\n\"\n",
        "                \"3. Try asking a different question about the video\\n\"\n",
        "                \"4. If issues persist, try re-uploading the video\\n\\n\"\n",
        "                \"**What I can help with:**\\n\"\n",
        "                \"- Video content analysis and description\\n\"\n",
        "                \"- Technical metadata extraction\\n\"\n",
        "                \"- Comprehensive video summaries\\n\"\n",
        "                \"- Anomaly detection in videos\\n\"\n",
        "                \"- Step-by-step video breakdowns\\n\"\n",
        "                \"\\nPlease try again with a video upload and your question.\"\n",
        "            )\n",
        "            return {**state, \"messages\": [AIMessage(content=response)]}\n",
        "\n",
        "        # Try to answer with the router LLM (Phi-3) using prior context\n",
        "        answer = None\n",
        "        try:\n",
        "            if self.router and self.router.model is not None and self.router.tokenizer is not None:\n",
        "                # Gather brief context (previous user/assistant turns)\n",
        "                ctx = []\n",
        "                for m in state.get(\"messages\", [])[:-1]:\n",
        "                    c = getattr(m, \"content\", \"\")\n",
        "                    if c:\n",
        "                        ctx.append(c)\n",
        "                context_text = \"\\n\".join(ctx)[-2000:]  # keep prompt small\n",
        "\n",
        "                prompt = (\n",
        "                    \"You are a helpful assistant. Using the conversation context (which may include a \"\n",
        "                    \"user-provided scene description), answer the latest question concisely and directly.\\n\\n\"\n",
        "                    f\"Context:\\n{context_text}\\n\\n\"\n",
        "                    f\"Question: {question}\\nAnswer:\"\n",
        "                )\n",
        "                toks = self.router.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024, padding=True)\n",
        "                if torch.cuda.is_available():\n",
        "                    toks = {k: v.to(self.router.device) for k, v in toks.items()}\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    out = self.router.model.generate(\n",
        "                        **toks,\n",
        "                        max_new_tokens=180,\n",
        "                        do_sample=False,\n",
        "                        pad_token_id=self.router.tokenizer.eos_token_id,\n",
        "                        eos_token_id=self.router.tokenizer.eos_token_id,\n",
        "                        use_cache=True,\n",
        "                    )\n",
        "                gen = out[0, toks[\"input_ids\"].shape[1]:]\n",
        "                answer = self.router.tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
        "        except Exception:\n",
        "            answer = None\n",
        "\n",
        "        if not answer:\n",
        "            # question-aware fallback (better than static help)\n",
        "            ql = (question or \"\").lower()\n",
        "            if any(k in ql for k in [\"how many\", \"count\", \"number of\"]):\n",
        "                answer = \"If you upload the video, I can count precisely. If you describe the scene, I can estimate from your text.\"\n",
        "            elif any(k in ql for k in [\"how long\", \"duration\", \"length\"]):\n",
        "                answer = \"I need the video or its metadata to give the exact duration.\"\n",
        "            else:\n",
        "                answer = \"Tell me what’s in the scene (or upload the video), and ask a specific question—I’ll answer directly.\"\n",
        "\n",
        "        return {**state, \"messages\": [AIMessage(content=answer)]}\n",
        "\n",
        "# Main Agent Class\n",
        "class VideoLlavaAgent:\n",
        "    \"\"\"Main VideoLlava agent with MCP integration\"\"\"\n",
        "\n",
        "    def __init__(self, model_config: ModelConfig = None, agent_config: AgentConfig = None):\n",
        "        self.model_config = model_config or ModelConfig()\n",
        "        self.agent_config = agent_config or AgentConfig()\n",
        "\n",
        "        # Initialize components\n",
        "        logger.info(\"Initializing VideoLlava Agent...\")\n",
        "\n",
        "        self.mcp_manager = MCPToolManager()\n",
        "        self.router = LLMRouter(self.model_config)\n",
        "        self.tools = VideoAnalysisTools(self.model_config, self.mcp_manager)\n",
        "        self.nodes = AgentNodes(self.tools, self.router, self.agent_config)\n",
        "\n",
        "        # Create workflow\n",
        "        self.workflow = self._create_workflow()\n",
        "\n",
        "        logger.info(\"VideoLlava Agent initialized successfully\")\n",
        "\n",
        "    def _create_workflow(self) -> StateGraph:\n",
        "        \"\"\"Create LangGraph workflow\"\"\"\n",
        "        workflow = StateGraph(AgentState)\n",
        "\n",
        "        # Add nodes\n",
        "        workflow.add_node(\"route\", lambda state: state)  # Routing happens in conditional edges\n",
        "        workflow.add_node(\"analyze_video\", self.nodes.analyze_video_node)\n",
        "        workflow.add_node(\"list_tools\", self.nodes.list_tools_node)\n",
        "        workflow.add_node(\"respond\", self.nodes.respond_node)\n",
        "\n",
        "        # Set up routing\n",
        "        workflow.add_conditional_edges(\n",
        "            START,\n",
        "            self.nodes.route_request,\n",
        "            {\n",
        "                \"analyze_video\": \"analyze_video\",\n",
        "                \"list_tools\": \"list_tools\",\n",
        "                \"respond\": \"respond\"\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Add endings\n",
        "        workflow.add_edge(\"analyze_video\", END)\n",
        "        workflow.add_edge(\"list_tools\", END)\n",
        "        workflow.add_edge(\"respond\", END)\n",
        "\n",
        "        # Add memory if enabled\n",
        "        if self.agent_config.enable_memory:\n",
        "            memory = MemorySaver()\n",
        "            return workflow.compile(checkpointer=memory)\n",
        "        else:\n",
        "            return workflow.compile()\n",
        "\n",
        "    async def initialize_mcp_servers(self, server_configs: List[Dict[str, Any]]):\n",
        "        \"\"\"Initialize MCP servers for external tool integration\"\"\"\n",
        "        for config in server_configs:\n",
        "            success = await self.mcp_manager.initialize_mcp_server(\n",
        "                config[\"name\"],\n",
        "                config[\"command\"]\n",
        "            )\n",
        "            if success:\n",
        "                logger.info(f\"MCP server {config['name']} initialized\")\n",
        "            else:\n",
        "                logger.warning(f\"Failed to initialize MCP server {config['name']}\")\n",
        "\n",
        "    def process_request(self, video_path: str, message: str, session_id: str = \"default\") -> Dict[str, Any]:\n",
        "        \"\"\"Process a user request with error handling\"\"\"\n",
        "        try:\n",
        "            # Prepare state\n",
        "            state = {\n",
        "                \"messages\": [HumanMessage(content=message)],\n",
        "                \"video_path\": video_path,\n",
        "                \"current_task\": message,\n",
        "                \"tool_calls\": [],\n",
        "                \"context\": {},\n",
        "                \"llm_classification\": None,\n",
        "                \"error_context\": None,\n",
        "                \"session_id\": session_id,\n",
        "                \"reasoning_trace\": []\n",
        "            }\n",
        "\n",
        "            # Run workflow\n",
        "            config = RunnableConfig({\"thread_id\": session_id})\n",
        "            result = self.workflow.invoke(state, config)\n",
        "\n",
        "            # Extract response\n",
        "            response = None\n",
        "            for msg in reversed(result[\"messages\"]):\n",
        "                if isinstance(msg, AIMessage):\n",
        "                    response = msg.content\n",
        "                    break\n",
        "\n",
        "            return {\n",
        "                \"response\": response or \"No response generated\",\n",
        "                \"tool_calls\": result.get(\"tool_calls\", []),\n",
        "                \"reasoning_trace\": result.get(\"reasoning_trace\", []),\n",
        "                \"error_context\": result.get(\"error_context\"),\n",
        "                \"classification\": result.get(\"llm_classification\")\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Request processing error: {e}\")\n",
        "            return {\n",
        "                \"response\": f\"Error processing request: {str(e)}\",\n",
        "                \"tool_calls\": [],\n",
        "                \"reasoning_trace\": [],\n",
        "                \"error_context\": {\"error\": str(e), \"stage\": \"processing\"},\n",
        "                \"classification\": None\n",
        "            }\n"
      ],
      "metadata": {
        "id": "6VUyGsO43uh9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "def load_models_with_progress():\n",
        "    \"\"\"Load models with detailed progress tracking\"\"\"\n",
        "\n",
        "    print(\"=== Model Loading Phase ===\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Determine optimal configuration based on available resources\n",
        "    if gpu_memory >= 20:\n",
        "        config_name = \"High-End\"\n",
        "        max_frames = 32\n",
        "        router_model = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "        max_tokens = 256\n",
        "    elif gpu_memory >= 12:\n",
        "        config_name = \"Mid-Range\"\n",
        "        max_frames = 16\n",
        "        router_model = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "        max_tokens = 200\n",
        "    else:\n",
        "        config_name = \"Low-Memory\"\n",
        "        max_frames = 8\n",
        "        router_model = \"microsoft/DialoGPT-medium\"\n",
        "        max_tokens = 150\n",
        "\n",
        "    print(f\"🎯 Using {config_name} Configuration\")\n",
        "    print(f\"   Max Frames: {max_frames}\")\n",
        "    print(f\"   Router Model: {router_model}\")\n",
        "    print(f\"   Max Tokens: {max_tokens}\")\n",
        "\n",
        "    # Create configurations\n",
        "    model_config = ModelConfig(\n",
        "        videollava_model=\"LanguageBind/Video-LLaVA-7B-hf\",\n",
        "        router_model=router_model,\n",
        "        device=\"cuda\" if gpu_available else \"cpu\",\n",
        "        torch_dtype=torch.float16 if gpu_available else torch.float32,\n",
        "        max_frames=max_frames,\n",
        "        max_new_tokens=max_tokens\n",
        "    )\n",
        "\n",
        "    agent_config = AgentConfig(\n",
        "        enable_reasoning=True,\n",
        "        enable_memory=True,\n",
        "        max_tool_calls=3 if gpu_memory >= 16 else 2,\n",
        "        fallback_enabled=True,\n",
        "        debug_mode=True\n",
        "    )\n",
        "\n",
        "    print(f\"⏳ Starting model loading at {datetime.now().strftime('%H:%M:%S')}\")\n",
        "\n",
        "    try:\n",
        "        # Create agent with progress tracking\n",
        "        print(\"🔄 Creating VideoLlava Agent...\")\n",
        "        agent = VideoLlavaAgent(model_config, agent_config)\n",
        "\n",
        "        load_time = time.time() - start_time\n",
        "        print(f\"✅ Models loaded successfully in {load_time:.1f} seconds\")\n",
        "\n",
        "        # Memory status after loading\n",
        "        if gpu_available:\n",
        "            allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
        "            cached = torch.cuda.memory_reserved(0) / 1024**3\n",
        "            print(f\"📊 GPU Memory - Allocated: {allocated:.1f}GB, Cached: {cached:.1f}GB\")\n",
        "\n",
        "        return agent, model_config, agent_config\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading models: {e}\")\n",
        "\n",
        "        # Try fallback configuration\n",
        "        print(\"🔄 Attempting fallback configuration...\")\n",
        "\n",
        "        fallback_config = ModelConfig(\n",
        "            videollava_model=\"LanguageBind/Video-LLaVA-7B-hf\",\n",
        "            router_model=\"microsoft/DialoGPT-medium\",\n",
        "            max_frames=6,\n",
        "            max_new_tokens=128\n",
        "        )\n",
        "\n",
        "        fallback_agent_config = AgentConfig(\n",
        "            enable_reasoning=False,\n",
        "            enable_memory=False,\n",
        "            max_tool_calls=1,\n",
        "            debug_mode=False\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            agent = VideoLlavaAgent(fallback_config, fallback_agent_config)\n",
        "            print(\"✅ Fallback configuration loaded\")\n",
        "            return agent, fallback_config, fallback_agent_config\n",
        "        except Exception as fallback_error:\n",
        "            print(f\"❌ Fallback also failed: {fallback_error}\")\n",
        "            raise\n",
        "\n",
        "# Load the models\n",
        "agent, model_config, agent_config = load_models_with_progress()"
      ],
      "metadata": {
        "id": "o6paJHjYTf1v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440,
          "referenced_widgets": [
            "215a6f0f729c4c41937c1d46425a79d2",
            "fc0e0485e66c4d61a8880b3a459d96c0",
            "47e84abaf97c4d4baaee6d1b707588e9",
            "bcb586601ba84d1aad851d4825f2b218",
            "966e96d4e4a448a2b6a5df5f35676d66",
            "e6cab3d77ee04f60bd251b0a74e75421",
            "aac258b9c7354b319a3f45bc93888f70",
            "48028d23f83d42ba92f9d01198927969",
            "f39992ec5b424e5eb0c19414ffc97531",
            "f42f1cb622d84a03b87397331b6970c8",
            "4f219fd2e7a84fa49263f9c729bd07ee",
            "7081e80495d0401aab089de22836c499",
            "907431dc830342a7bc216b9657cb942d",
            "cb710d330ca044888e5e24cca0f27751",
            "6f358c3522c04464880e06c6b6145db0",
            "e459d41bf78844149fad4563434fa594",
            "622f52c2e9504e9a97e730bad148a64e",
            "d1ad94b0fa7d43d68fde842e7cd30133",
            "f8566ca5225e4be194ff33a4be4a4ff5",
            "f070f6e1972f4370afb02e545f1e0854",
            "fbd22e55dfb34834bebf8b8e0f059e8c",
            "fd2ef6269c0b4f3bbfc20f42a7a32a67"
          ]
        },
        "outputId": "6a41a9dc-c37d-4325-bef1-f5c3e2fc5e63"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Model Loading Phase ===\n",
            "🎯 Using High-End Configuration\n",
            "   Max Frames: 32\n",
            "   Router Model: microsoft/Phi-3-mini-4k-instruct\n",
            "   Max Tokens: 256\n",
            "⏳ Starting model loading at 04:39:31\n",
            "🔄 Creating VideoLlava Agent...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "215a6f0f729c4c41937c1d46425a79d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n",
            "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7081e80495d0401aab089de22836c499"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Models loaded successfully in 14.0 seconds\n",
            "📊 GPU Memory - Allocated: 20.8GB, Cached: 21.4GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio Interface for VideoLlava Agent\n",
        "# Optimized for Google Colab deployment with better memory management\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "import logging\n",
        "from typing import List, Tuple, Optional, Dict, Any, Union\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "import os\n",
        "import gc\n",
        "\n",
        "# Setup logging for Gradio interface\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class VideoLlavaInterface:\n",
        "    \"\"\"Gradio interface with memory management and session handling\"\"\"\n",
        "\n",
        "    def __init__(self, agent: 'VideoLlavaAgent'):\n",
        "        self.agent = agent\n",
        "        self.current_video_path = None\n",
        "        self.session_history = {}\n",
        "        self.performance_stats = {\"total_requests\": 0, \"avg_response_time\": 0}\n",
        "\n",
        "    def process_video_message(\n",
        "        self,\n",
        "        video,\n",
        "        message: str,\n",
        "        history: Optional[List[Dict[str, str]]] = None,\n",
        "        session_id: str = \"default\",\n",
        "    ) -> Tuple[List[Dict[str, str]], str]:\n",
        "        start_time = time.time()\n",
        "        history = history or []\n",
        "\n",
        "        if not message.strip():\n",
        "            return history, \"\"\n",
        "\n",
        "        # update video path if new upload\n",
        "        if video and video != self.current_video_path:\n",
        "            self.current_video_path = video\n",
        "            logger.info(f\"New video uploaded: {os.path.basename(video) if video else 'None'}\")\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "        # add user message\n",
        "        history.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "        try:\n",
        "            result = self.agent.process_request(\n",
        "                video_path=self.current_video_path,\n",
        "                message=message,\n",
        "                session_id=session_id\n",
        "            )\n",
        "\n",
        "            response = result[\"response\"]\n",
        "\n",
        "            # optional metadata\n",
        "            if result.get(\"classification\"):\n",
        "                classification = result[\"classification\"]\n",
        "                confidence = classification.get(\"confidence\", \"Unknown\")\n",
        "                tool_used = classification.get(\"primary_tool\", \"Unknown\")\n",
        "                metadata = \"\\n\\n**Analysis Details:**\\n\"\n",
        "                metadata += f\"- **Tool Selected**: {tool_used.replace('_', ' ').title()}\\n\"\n",
        "                metadata += f\"- **Confidence**: {confidence}\\n\"\n",
        "                if result.get(\"reasoning_trace\"):\n",
        "                    metadata += f\"- **Reasoning**: {result['reasoning_trace'][-1]}\\n\"\n",
        "                response += metadata\n",
        "\n",
        "            if self.current_video_path:\n",
        "                response += f\"\\n\\n**Current Video**: {os.path.basename(self.current_video_path)}\"\n",
        "\n",
        "            # add assistant message\n",
        "            history.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "            # perf stats\n",
        "            self._update_performance_stats(time.time() - start_time)\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            return history, \"\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing message: {e}\")\n",
        "            error_response = f\"**Error**: {str(e)}\\n\\n\"\n",
        "            if self.current_video_path:\n",
        "                error_response += f\"Video loaded: {os.path.basename(self.current_video_path)}\\n\"\n",
        "            else:\n",
        "                error_response += \"No video currently loaded. Please upload a video first.\\n\"\n",
        "            error_response += \"\\n**Troubleshooting:**\\n- Ensure video file is properly uploaded\\n- Check video format (MP4, AVI, MOV supported)\\n- Try restarting if memory issues persist\\n\"\n",
        "\n",
        "            history.append({\"role\": \"assistant\", \"content\": error_response})\n",
        "            return history, \"\"\n",
        "\n",
        "    def clear_session(self) -> Tuple[List, str]:\n",
        "        \"\"\"Clear session and reset video state\"\"\"\n",
        "        logger.info(\"Clearing session and resetting state\")\n",
        "\n",
        "        self.current_video_path = None\n",
        "\n",
        "        # Force garbage collection and GPU memory cleanup\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        return [], \"\"\n",
        "\n",
        "    def get_system_status(self) -> str:\n",
        "        \"\"\"Get current system status and performance info\"\"\"\n",
        "        status = \"**System Status**\\n\\n\"\n",
        "\n",
        "        # GPU Information\n",
        "        if torch.cuda.is_available():\n",
        "            gpu_name = torch.cuda.get_device_name(0)\n",
        "            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "            allocated_memory = torch.cuda.memory_allocated(0) / 1024**3\n",
        "            cached_memory = torch.cuda.memory_reserved(0) / 1024**3\n",
        "\n",
        "            status += f\"**GPU**: {gpu_name}\\n\"\n",
        "            status += f\"**Memory**: {allocated_memory:.1f}GB / {total_memory:.1f}GB allocated\\n\"\n",
        "            status += f\"**Cached**: {cached_memory:.1f}GB\\n\"\n",
        "        else:\n",
        "            status += \"**Device**: CPU only\\n\"\n",
        "\n",
        "        # Agent Status\n",
        "        status += f\"\\n**Agent Status**:\\n\"\n",
        "        status += f\"- **LLM Router**: {'Active' if self.agent.router.model else 'Fallback Mode'}\\n\"\n",
        "        status += f\"- **VideoLlava**: {'Loaded' if self.agent.tools.model else 'Not Loaded'}\\n\"\n",
        "        status += f\"- **MCP Integration**: {'Available' if self.agent.mcp_manager.mcp_enabled else 'Not Available'}\\n\"\n",
        "\n",
        "        # Current Session\n",
        "        status += f\"\\n**Current Session**:\\n\"\n",
        "        status += f\"- **Video Loaded**: {'Yes' if self.current_video_path else 'No'}\\n\"\n",
        "        if self.current_video_path:\n",
        "            status += f\"- **Video File**: {os.path.basename(self.current_video_path)}\\n\"\n",
        "\n",
        "        # Performance Stats\n",
        "        status += f\"\\n**Performance**:\\n\"\n",
        "        status += f\"- **Total Requests**: {self.performance_stats['total_requests']}\\n\"\n",
        "        status += f\"- **Avg Response Time**: {self.performance_stats['avg_response_time']:.2f}s\\n\"\n",
        "\n",
        "        return status\n",
        "\n",
        "    def _update_performance_stats(self, response_time: float):\n",
        "        \"\"\"Update performance statistics\"\"\"\n",
        "        current_total = self.performance_stats[\"total_requests\"]\n",
        "        current_avg = self.performance_stats[\"avg_response_time\"]\n",
        "\n",
        "        new_total = current_total + 1\n",
        "        new_avg = (current_avg * current_total + response_time) / new_total\n",
        "\n",
        "        self.performance_stats[\"total_requests\"] = new_total\n",
        "        self.performance_stats[\"avg_response_time\"] = new_avg\n",
        "\n",
        "def create_gradio_interface(agent: 'VideoLlavaAgent') -> gr.Blocks:\n",
        "    \"\"\"Create comprehensive Gradio interface\"\"\"\n",
        "\n",
        "    interface = VideoLlavaInterface(agent)\n",
        "\n",
        "    # Custom CSS for better styling\n",
        "    custom_css = \"\"\"\n",
        "    .gradio-container {\n",
        "        max-width: 1200px !important;\n",
        "    }\n",
        "    .video-upload {\n",
        "        border: 2px dashed #4A90E2;\n",
        "        border-radius: 10px;\n",
        "        padding: 20px;\n",
        "    }\n",
        "    .status-box {\n",
        "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "        padding: 15px;\n",
        "        border-radius: 10px;\n",
        "        color: white;\n",
        "        margin: 10px 0;\n",
        "    }\n",
        "    .tool-info {\n",
        "        background-color: #f8f9fa;\n",
        "        padding: 15px;\n",
        "        border-radius: 8px;\n",
        "        border-left: 4px solid #4A90E2;\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    with gr.Blocks(\n",
        "        title=\"VideoLlava Agent\",\n",
        "        theme=gr.themes.Soft(),\n",
        "        css=custom_css\n",
        "    ) as demo:\n",
        "\n",
        "        # Header\n",
        "        gr.HTML(\"\"\"\n",
        "        <div style='text-align: center; margin-bottom: 20px;'>\n",
        "            <h1 style='color: #4A90E2; margin-bottom: 10px;'>VideoLlava Agent</h1>\n",
        "            <p style='font-size: 18px; color: #666;'>\n",
        "                Intelligent Video Analysis with LLM Routing + LangGraph + VideoLlava\n",
        "            </p>\n",
        "        </div>\n",
        "        \"\"\")\n",
        "\n",
        "        # System Status\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=2):\n",
        "                system_status = gr.HTML(\n",
        "                    value=interface.get_system_status()\n",
        "                )\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                refresh_status_btn = gr.Button(\"Refresh Status\", variant=\"secondary\")\n",
        "                clear_memory_btn = gr.Button(\"Clear Memory\", variant=\"secondary\")\n",
        "\n",
        "        gr.HTML(\"<hr>\")\n",
        "\n",
        "        # Main Interface\n",
        "        with gr.Row():\n",
        "            # Left Column - Video Upload and Instructions\n",
        "            with gr.Column(scale=1):\n",
        "                video_input = gr.Video(\n",
        "                    label=\"Upload Video for Analysis\",\n",
        "                    height=350\n",
        "                )\n",
        "\n",
        "                # Instructions\n",
        "                gr.HTML(\"\"\"\n",
        "                <div class=\"tool-info\">\n",
        "                    <h4>How to Use</h4>\n",
        "                    <ol>\n",
        "                        <li><strong>Upload Video</strong>: Select any video file (MP4, AVI, MOV, etc.)</li>\n",
        "                        <li><strong>Ask Questions</strong>: Use natural language to ask about the video</li>\n",
        "                        <li><strong>Get Analysis</strong>: The AI will automatically select the best tool</li>\n",
        "                    </ol>\n",
        "                </div>\n",
        "                \"\"\")\n",
        "\n",
        "                # Example Questions\n",
        "                with gr.Accordion(\"Example Questions\", open=False):\n",
        "                    gr.HTML(\"\"\"\n",
        "                    <div style=\"padding: 10px;\">\n",
        "                        <p><strong>Content Analysis:</strong></p>\n",
        "                        <ul>\n",
        "                            <li>\"What's happening in this video?\"</li>\n",
        "                            <li>\"How many people are there?\"</li>\n",
        "                            <li>\"Describe what you see\"</li>\n",
        "                        </ul>\n",
        "\n",
        "                        <p><strong>Technical Analysis:</strong></p>\n",
        "                        <ul>\n",
        "                            <li>\"How long is this video?\"</li>\n",
        "                            <li>\"What's the resolution and frame rate?\"</li>\n",
        "                            <li>\"Give me technical details\"</li>\n",
        "                        </ul>\n",
        "\n",
        "                        <p><strong>Advanced Analysis:</strong></p>\n",
        "                        <ul>\n",
        "                            <li>\"Give me a comprehensive analysis\"</li>\n",
        "                            <li>\"Is anything unusual happening?\"</li>\n",
        "                            <li>\"Break this down step by step\"</li>\n",
        "                        </ul>\n",
        "                    </div>\n",
        "                    \"\"\")\n",
        "\n",
        "                # Performance Monitor\n",
        "                with gr.Accordion(\"Performance Monitor\", open=False):\n",
        "                    performance_display = gr.JSON(\n",
        "                        value=interface.performance_stats,\n",
        "                        label=\"Performance Statistics\"\n",
        "                    )\n",
        "\n",
        "                    perf_timer = gr.Timer(30.0)\n",
        "                    perf_timer.tick(lambda: interface.performance_stats, None, performance_display)\n",
        "\n",
        "            # Right Column - Chat Interface\n",
        "            with gr.Column(scale=2):\n",
        "                chatbot = gr.Chatbot(\n",
        "                    label=\"VideoLlava Agent Chat\",\n",
        "                    height=500,\n",
        "                    show_copy_button=True,\n",
        "                    type=\"messages\"\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    msg_input = gr.Textbox(\n",
        "                        label=\"Ask about your video\",\n",
        "                        placeholder=\"Upload a video and ask any question - the AI will select the best analysis method!\",\n",
        "                        lines=2,\n",
        "                        scale=4\n",
        "                    )\n",
        "\n",
        "                with gr.Row():\n",
        "                    send_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
        "                    clear_btn = gr.Button(\"Clear Chat\", variant=\"secondary\", scale=1)\n",
        "\n",
        "        # Advanced Settings\n",
        "        with gr.Accordion(\"Advanced Settings\", open=False):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    session_id_input = gr.Textbox(\n",
        "                        value=\"default\",\n",
        "                        label=\"Session ID\",\n",
        "                        placeholder=\"default\"\n",
        "                    )\n",
        "\n",
        "                with gr.Column():\n",
        "                    debug_mode = gr.Checkbox(\n",
        "                        value=agent.agent_config.debug_mode,\n",
        "                        label=\"Debug Mode\"\n",
        "                    )\n",
        "\n",
        "                with gr.Column():\n",
        "                    max_frames = gr.Slider(\n",
        "                        minimum=8,\n",
        "                        maximum=64,\n",
        "                        step=8,\n",
        "                        value=agent.model_config.max_frames,\n",
        "                        label=\"Max Frames for Analysis\"\n",
        "                    )\n",
        "\n",
        "        # Event Handlers\n",
        "        def send_message(video, message, history, session_id):\n",
        "            return interface.process_video_message(video, message, history, session_id)[:2]\n",
        "\n",
        "        def update_system_status():\n",
        "            return interface.get_system_status()\n",
        "\n",
        "        def clear_memory():\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            return \"Memory cleared successfully\"\n",
        "\n",
        "        # Connect events\n",
        "        send_btn.click(\n",
        "            fn=send_message,\n",
        "            inputs=[video_input, msg_input, chatbot, session_id_input],\n",
        "            outputs=[chatbot, msg_input],\n",
        "            show_progress=True\n",
        "        )\n",
        "\n",
        "        msg_input.submit(\n",
        "            fn=send_message,\n",
        "            inputs=[video_input, msg_input, chatbot, session_id_input],\n",
        "            outputs=[chatbot, msg_input],\n",
        "            show_progress=True\n",
        "        )\n",
        "\n",
        "        clear_btn.click(\n",
        "            fn=interface.clear_session,\n",
        "            outputs=[chatbot, msg_input]\n",
        "        )\n",
        "\n",
        "        refresh_status_btn.click(\n",
        "            fn=update_system_status,\n",
        "            outputs=system_status\n",
        "        )\n",
        "\n",
        "        clear_memory_btn.click(\n",
        "            fn=clear_memory,\n",
        "            outputs=gr.Textbox(visible=False)  # Hidden output\n",
        "        )\n",
        "\n",
        "        # Auto-refresh performance stats\n",
        "        demo.load(\n",
        "            fn=lambda: interface.performance_stats,\n",
        "            outputs=performance_display\n",
        "        )\n",
        "\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Colab-specific setup and deployment\n",
        "def setup_for_colab():\n",
        "    \"\"\"Setup optimized for Google Colab environment\"\"\"\n",
        "\n",
        "    print(\"Setting up VideoLlava Agent for Google Colab...\")\n",
        "\n",
        "    # Memory optimization\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        # Set memory fraction to prevent OOM\n",
        "        torch.cuda.set_per_process_memory_fraction(0.85)\n",
        "\n",
        "    # Configure for Colab\n",
        "    os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")  # Avoid tokenizer warnings\n",
        "\n",
        "    # Model configuration optimized for Colab\n",
        "    model_config = ModelConfig(\n",
        "        videollava_model=\"LanguageBind/Video-LLaVA-7B-hf\",\n",
        "        router_model=\"microsoft/Phi-3-mini-4k-instruct\",\n",
        "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "        max_frames=16,  # Reduced for Colab\n",
        "        max_new_tokens=200  # Reduced for faster generation\n",
        "    )\n",
        "\n",
        "    # Agent configuration\n",
        "    agent_config = AgentConfig(\n",
        "        enable_reasoning=True,\n",
        "        enable_memory=True,\n",
        "        max_tool_calls=2,  # Reduced for Colab\n",
        "        fallback_enabled=True,\n",
        "        debug_mode=True\n",
        "    )\n",
        "\n",
        "    return model_config, agent_config\n",
        "\n",
        "def deploy_in_colab():\n",
        "    \"\"\"Complete deployment function for Colab\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Setup configurations\n",
        "        model_config, agent_config = setup_for_colab()\n",
        "\n",
        "        print(\"Creating VideoLlava Agent...\")\n",
        "        agent = VideoLlavaAgent(model_config, agent_config)\n",
        "\n",
        "        print(\"Creating Gradio interface...\")\n",
        "        demo = create_gradio_interface(agent)\n",
        "\n",
        "        print(\"Launching interface...\")\n",
        "        demo.launch(\n",
        "            share=True,\n",
        "            server_port=7860,\n",
        "            server_name=\"0.0.0.0\",\n",
        "            show_error=True,\n",
        "            debug=True,\n",
        "            quiet=False\n",
        "        )\n",
        "\n",
        "        return demo, agent\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during deployment: {e}\")\n",
        "\n",
        "        # Fallback: Try with reduced configuration\n",
        "        print(\"Attempting fallback configuration...\")\n",
        "\n",
        "        model_config = ModelConfig(\n",
        "            videollava_model=\"LanguageBind/Video-LLaVA-7B-hf\",\n",
        "            router_model=\"microsoft/DialoGPT-medium\",  # Smaller fallback\n",
        "            max_frames=8,\n",
        "            max_new_tokens=150\n",
        "        )\n",
        "\n",
        "        agent_config = AgentConfig(\n",
        "            enable_reasoning=False,  # Disable for simplicity\n",
        "            enable_memory=False,\n",
        "            max_tool_calls=1\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            agent = VideoLlavaAgent(model_config, agent_config)\n",
        "            demo = create_gradio_interface(agent)\n",
        "            demo.launch(share=True, server_port=7860, server_name=\"0.0.0.0\")\n",
        "\n",
        "            return demo, agent\n",
        "\n",
        "        except Exception as fallback_error:\n",
        "            print(f\"Fallback also failed: {fallback_error}\")\n",
        "            raise\n"
      ],
      "metadata": {
        "id": "qSXe-RE2VJf_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_optimized_interface(agent):\n",
        "    \"\"\"Create interface optimized for Colab\"\"\"\n",
        "\n",
        "    print(\"🎨 Creating Gradio interface...\")\n",
        "\n",
        "    # Import the interface creation function and modify for Colab\n",
        "    interface = VideoLlavaInterface(agent)\n",
        "\n",
        "    # Create custom Gradio interface for Colab\n",
        "    custom_css = \"\"\"\n",
        "    .gradio-container {\n",
        "        max-width: 100% !important;\n",
        "        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
        "    }\n",
        "    .video-upload {\n",
        "        border: 3px dashed #4CAF50;\n",
        "        border-radius: 15px;\n",
        "        padding: 20px;\n",
        "        background: linear-gradient(145deg, #f0f8ff, #e6f3ff);\n",
        "    }\n",
        "    .status-display {\n",
        "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "        color: white;\n",
        "        padding: 15px;\n",
        "        border-radius: 10px;\n",
        "        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n",
        "    }\n",
        "    .chat-container {\n",
        "        border-radius: 10px;\n",
        "        box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    with gr.Blocks(\n",
        "        title=\"VideoLlava Agent - Colab Edition\",\n",
        "        theme=gr.themes.Soft(primary_hue=\"blue\", secondary_hue=\"purple\"),\n",
        "        css=custom_css\n",
        "    ) as demo:\n",
        "\n",
        "        # Header with system info\n",
        "        gpu_status = \"🟢 GPU Ready\" if gpu_available else \"🔴 CPU Mode\"\n",
        "        router_status = \"🧠 LLM Router Active\" if agent.router.model else \"⚠️ Pattern Matching\"\n",
        "\n",
        "        gr.HTML(f\"\"\"\n",
        "        <div style='text-align: center; padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "                    color: white; border-radius: 15px; margin-bottom: 20px; box-shadow: 0 4px 15px rgba(0,0,0,0.2);'>\n",
        "            <h1 style='margin: 0; font-size: 2.5em; font-weight: bold;'>VideoLlava Agent</h1>\n",
        "            <p style='margin: 10px 0; font-size: 1.2em;'>Intelligent Video Analysis • LangGraph Orchestration • Colab Optimized</p>\n",
        "            <div style='margin-top: 15px;'>\n",
        "                <span style='margin: 0 15px; padding: 5px 15px; background: rgba(255,255,255,0.2);\n",
        "                           border-radius: 20px; font-weight: bold;'>{gpu_status}</span>\n",
        "                <span style='margin: 0 15px; padding: 5px 15px; background: rgba(255,255,255,0.2);\n",
        "                           border-radius: 20px; font-weight: bold;'>{router_status}</span>\n",
        "                <span style='margin: 0 15px; padding: 5px 15px; background: rgba(255,255,255,0.2);\n",
        "                           border-radius: 20px; font-weight: bold;'>🛠️ 6 Analysis Tools</span>\n",
        "            </div>\n",
        "        </div>\n",
        "        \"\"\")\n",
        "\n",
        "        # Quick Stats Row\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gr.HTML(f\"\"\"\n",
        "                <div class=\"status-display\">\n",
        "                    <h4>⚡ Performance Config</h4>\n",
        "                    <p><strong>Max Frames:</strong> {model_config.max_frames}</p>\n",
        "                    <p><strong>Max Tokens:</strong> {model_config.max_new_tokens}</p>\n",
        "                    <p><strong>Memory Mode:</strong> {'Optimized' if gpu_available else 'Conservation'}</p>\n",
        "                </div>\n",
        "                \"\"\")\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                system_monitor = gr.HTML(\n",
        "                    value=interface.get_system_status(),\n",
        "                )\n",
        "\n",
        "                refresh_btn = gr.Button(\"🔄 Refresh Status\")\n",
        "\n",
        "        # Main Interface\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                # Video Upload\n",
        "                video_input = gr.Video(\n",
        "                    label=\"📹 Upload Video for AI Analysis\",\n",
        "                    height=350\n",
        "                )\n",
        "\n",
        "                # Quick Action Buttons\n",
        "                gr.HTML(\"<h4>🚀 Quick Actions</h4>\")\n",
        "                with gr.Row():\n",
        "                    analyze_btn = gr.Button(\"🔍 Analyze Content\", variant=\"primary\")\n",
        "                    metadata_btn = gr.Button(\"📊 Get Metadata\", variant=\"secondary\")\n",
        "                    summary_btn = gr.Button(\"📝 Full Summary\", variant=\"secondary\")\n",
        "\n",
        "\n",
        "                # Example Questions\n",
        "                with gr.Accordion(\"💡 Example Questions\", open=True):\n",
        "                    example_questions = [\n",
        "                        \"What's happening in this video?\",\n",
        "                        \"How many people are there?\",\n",
        "                        \"How long is this video?\",\n",
        "                        \"Give me technical details\",\n",
        "                        \"Is anything unusual happening?\",\n",
        "                        \"Break this down step by step\"\n",
        "                    ]\n",
        "\n",
        "                    for question in example_questions:\n",
        "                        gr.Button(question, variant=\"secondary\")\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                # Chat Interface\n",
        "                chatbot = gr.Chatbot(\n",
        "                  label=\"🤖 VideoLlava Chat Assistant\",\n",
        "                  height=500,\n",
        "                  show_copy_button=True,\n",
        "                  type=\"messages\"\n",
        "                )\n",
        "\n",
        "                # Message Input\n",
        "                with gr.Row():\n",
        "                    msg_input = gr.Textbox(\n",
        "                        label=\"💬 Ask about your video\",\n",
        "                        placeholder=\"Upload a video and ask any question - I'll automatically select the best analysis method!\",\n",
        "                        lines=2,\n",
        "                        scale=5\n",
        "                    )\n",
        "\n",
        "                # Action Buttons\n",
        "                with gr.Row():\n",
        "                    send_btn = gr.Button(\"📤 Send\", variant=\"primary\", scale=2)\n",
        "                    clear_btn = gr.Button(\"🗑️ Clear Chat\", variant=\"secondary\", scale=1)\n",
        "                    memory_btn = gr.Button(\"💾 Clear Memory\", variant=\"secondary\", scale=1)\n",
        "\n",
        "        # Performance Monitor\n",
        "        with gr.Accordion(\"📈 Performance Monitor\", open=False):\n",
        "            with gr.Row():\n",
        "                performance_json = gr.JSON(label=\"Performance Stats\")\n",
        "                error_log = gr.Textbox(label=\"System Log\", lines=5, max_lines=10)\n",
        "                # Auto-refresh performance\n",
        "                perf_timer = gr.Timer(30.0)\n",
        "                perf_timer.tick(lambda: interface.performance_stats, None, performance_json)\n",
        "\n",
        "        # Event Handlers\n",
        "        def send_message(video, message, history):\n",
        "            return interface.process_video_message(video, message, history)[:2]\n",
        "\n",
        "        def quick_analyze(video, history):\n",
        "            if video:\n",
        "                return send_message(video, \"What's happening in this video?\", history)\n",
        "            return history, \"\"\n",
        "\n",
        "        def quick_metadata(video, history):\n",
        "            if video:\n",
        "                return send_message(video, \"Give me the technical details and metadata\", history)\n",
        "            return history, \"\"\n",
        "\n",
        "        def quick_summary(video, history):\n",
        "            if video:\n",
        "                return send_message(video, \"Provide a comprehensive analysis of this video\", history)\n",
        "            return history, \"\"\n",
        "\n",
        "        # Connect events\n",
        "        send_btn.click(send_message, [video_input, msg_input, chatbot], [chatbot, msg_input])\n",
        "        msg_input.submit(send_message, [video_input, msg_input, chatbot], [chatbot, msg_input])\n",
        "\n",
        "        analyze_btn.click(quick_analyze, [video_input, chatbot], [chatbot, msg_input])\n",
        "        metadata_btn.click(quick_metadata, [video_input, chatbot], [chatbot, msg_input])\n",
        "        summary_btn.click(quick_summary, [video_input, chatbot], [chatbot, msg_input])\n",
        "\n",
        "        clear_btn.click(interface.clear_session, outputs=[chatbot, msg_input])\n",
        "        refresh_btn.click(lambda: interface.get_system_status(), outputs=system_monitor)\n",
        "\n",
        "\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Create and launch interface\n",
        "print(\"🚀 Creating optimized interface...\")\n",
        "demo = create_optimized_interface(agent)\n",
        "\n",
        "print(\"🌐 Launching Gradio interface...\")\n",
        "demo.launch(\n",
        "    share=True,           # Create public link\n",
        "    debug=True,          # Enable debug mode\n",
        "    server_name=\"0.0.0.0\",  # Allow external access\n",
        "    server_port=7860,    # Standard port\n",
        "    show_error=True,     # Show detailed errors\n",
        "    inbrowser=True,      # Open in browser automatically\n",
        "    height=800,          # Set height\n",
        "    favicon_path=None,   # No custom favicon\n",
        "    auth=None,           # No authentication\n",
        "    max_threads=40       # Handle multiple users\n",
        ")"
      ],
      "metadata": {
        "id": "r_LEbYDqWXNq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 984
        },
        "outputId": "e1afe6dd-067f-4b80-a663-e54da60b0b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Creating optimized interface...\n",
            "🎨 Creating Gradio interface...\n",
            "🌐 Launching Gradio interface...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://759cd6d142d5ab79d6.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://759cd6d142d5ab79d6.gradio.live\" width=\"100%\" height=\"800\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        }
      ]
    }
  ]
}